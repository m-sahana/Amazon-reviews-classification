# -*- coding: utf-8 -*-
"""My_Project_28 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rB6c-1IsnGv6By9KbIQU4Np0dexOM9D4

Loading DataSet
"""

!pip install spacy
!python -m spacy download en_core_web_sm
!pip install beautifulsoup4
!pip install preprocessor
import preprocessor as ps
!pip install contractions
!pip install pycontractions
!pip install bert-for-tf2
!pip install sentencepiece
!pip install pdpipe 
!pip install symspellpy
import sys  
!{sys.executable} -m pip install contractions
import unicodedata
!pip install plotly
!pip install cufflinks
!pip install textblob
!pip install pyLDAvis==2.1.2

!pip install -U scikit-learn

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import word_tokenize
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk
import json
# %matplotlib inline
from bs4 import BeautifulSoup as bs #Beautiful Soup is a Python library for pulling data out of HTML and XML files.
import requests # making HTTP requests in Python
import re
import gzip
import itertools
import string
import wordcloud
import datetime as dt
import pylab as pl
import plotly as py
import cufflinks as cf
from plotly.offline import iplot
py.offline.init_notebook_mode(connected=True)
cf.go_offline()
import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import gensim
from gensim import corpora

from gensim.models.ldamulticore import LdaMulticore
from gensim import corpora, models
import pyLDAvis.gensim #LDA visualization library
from itertools import chain

product_data=pd.read_csv('Comp Product Reviews.csv')

product_data.head()

product_data=product_data.drop(['ID', 'profileName'], axis=1)
product_data

"""DATA INFO"""

product_data.shape

product_data.info()

"""We can see that we don't have any null values.

# Check for Duplicate entries
"""

# Check for Rows containing duplicate data
duplicate_rows_df = product_data[product_data.duplicated()]

duplicate_rows_df.shape

"""## Droping duplicates"""

PD_D=product_data.drop_duplicates()

product_data.count()

"""Checking for Null Values"""

# Finding the null values.
print(product_data.isnull().sum())

"""**Droping Null values**"""

product_data = product_data.dropna() 
product_data.count()

# After dropping the values check for Null Values
print(product_data.isnull().sum())

"""NO of Brands"""

print(product_data["Brand"].nunique())
product_data["Brand"].unique()

"""# **how many Brands of Airfryer have Maximum ratings**"""

print(product_data["Brand"][product_data["Rating"] == product_data["Rating"].max()].nunique())
print(print(product_data["Brand"][product_data["Rating"] == product_data["Rating"].max()].unique()))
print(product_data["Brand"][product_data["Rating"] == product_data["Rating"].max()].count())

"""How many brands has Mininum Ratings what are their names"""

print(product_data["Brand"][product_data["Rating"] == product_data["Rating"].min()].nunique())
print(print(product_data["Brand"][product_data["Rating"] == product_data["Rating"].min()].unique()))
print(product_data["Brand"][product_data["Rating"] == product_data["Rating"].min()].count())

"""# **Which Brand has Maximum and minimum Number of reviews**"""

product_data[["Brand", "Rating"]][product_data["ReviewTitle"] == product_data["ReviewTitle"].min()]

product_data[["Brand", "Rating"]][product_data["ReviewTitle"] == product_data["ReviewTitle"].max()]

#Sorting data according to ProductId in ascending order
product_data=product_data.sort_values("Rating",axis=0,ascending=True,inplace=False,kind='quicksort',na_position='last')
product_data.head()

print(product_data['Rating'].value_counts())
print(product_data['Brand'].value_counts())

ax = sns.countplot(x=product_data["Rating"],  data=product_data, order = product_data["Rating"].value_counts().index )
for p, label in zip(ax.patches, product_data["Rating"].value_counts()):
    print(p)
    print(label)
    ax.annotate(label, (p.get_x()+0.25, p.get_height()+0.5))

"""
Tell me brand that has maximum numbers of reviews and how many ?"""

df1=product_data.groupby('Brand').count()

df1["ReviewTitle"].plot(kind='bar')

plt.figure(figsize=(20,10))
sns.countplot(y="Brand",  hue="Rating", data=product_data)

"""**Text Cleaning and Text Preprocessing**

*For Text Preprocessing we will use TextBlob Library. In Text Preprocessing we remove stop words, punctuations, convert into lower cases, lemmatize,spell check TextBlob is built upon NLTK and provides an easy to use interface to the NLTK library. various tasks can be performed like part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.
*
"""

Z=''.join(product_data['ReviewTitle'].tolist())
Z

"""**Contractions are those little literary shortcuts we take where instead of “Should have” we prefer “Should’ve” or where “Do not” quickly becomes “Don’t”. We are going to add a new column to our dataframe called “no_contract” and apply a lambda function to the “Reviews” field which will expand any contractions.**"""

contractions = { "ain't": "are not","'s":" is","aren't": "are not",
                     "can't": "cannot","can't've": "cannot have",
                     "'cause": "because","could've": "could have","couldn't": "could not",
                     "couldn't've": "could not have", "didn't": "did not","doesn't": "does not",
                     "don't": "do not","hadn't": "had not","hadn't've": "had not have",
                     "hasn't": "has not","haven't": "have not","he'd": "he would",
                     "he'd've": "he would have","he'll": "he will", "he'll've": "he will have",
                     "how'd": "how did","how'd'y": "how do you","how'll": "how will",
                     "I'd": "I would", "I'd've": "I would have","I'll": "I will",
                     "I'll've": "I will have","I'm": "I am","I've": "I have", "isn't": "is not",
                     "it'd": "it would","it'd've": "it would have","it'll": "it will",
                     "it'll've": "it will have", "let's": "let us","ma'am": "madam",
                      "might've": "might have" ,"must've": "must have","o'clock": "of the clock",
                    "she'd": "she would","she'd've": "she would have",
                     "she'll": "she will", "she'll've": "she will have","should've": "should have",
                    "that'd": "that would","that'd've": "that would have", "there'd": "there would",
                     "there'd've": "there would have", "they'd": "they would",
                     "they'd've": "they would have","they'll": "they will",
                     "they'll've": "they will have", "they're": "they are","they've": "they have",
                     "to've": "to have","we'd": "we would",
                     "we'd've": "we would have","we'll": "we will","we'll've": "we will have",
                     "we're": "we are","we've": "we have", "weren't": "were not","what'll": "what will",
                     "what'll've": "what will have","what're": "what are", "what've": "what have",
                     "when've": "when have","where'd": "where did", "where've": "where have",
                     "who'll": "who will","who'll've": "who will have","who've": "who have",
                     "why've": "why have","will've": "will have",
                      "would've": "would have" ,"y'all": "you all", "y'all'd": "you all would",
                     "y'all'd've": "you all would have","y'all're": "you all are",
                     "y'all've": "you all have", "you'd": "you would","you'd've": "you would have",
                     "you'll": "you will","you'll've": "you will have", "you're": "you are",
                     "you've": "you have"}

def cont_to_exp(x):
  if type(x) is str:               ###We cannot apply the contractions when it is with Numerical number
      x=x.replace('\\','')
  for key in contractions:
    values=contractions[key]
    x=x.replace(key,values)
    return
  else:
    return x

# Regular expression for finding contractions
contractions_re=re.compile('(%s)' % '|'.join(contractions.keys()))
contractions_re

"""TEXT **PREPROCESSING**"""

# find sentences containing HTML tags
import re
i=0;
for sent in product_data['ReviewTitle'].values:
    if (len(re.findall('<.*?>', sent))):
        print(i)
        print(sent)
        break;
    i += 1;

####TOKENIZE#####################



for sent in product_data['ReviewTitle'].values:
  def cleandurl(sent): #function to clean the word of any URL
    cleandu = re.compile('r"http\S+"')
    cleanurl= re.sub(cleandu, ' ', sent)
    return cleanurl

for sent in product_data['ReviewTitle'].values:
  def cleanlower(sent): #function to clean and lower the sencentce
     cleandl = re.compile('lower()')
     cleandlr= re.sub(cleandl, ' ', sent)
     return cleandlr
for sent in product_data['ReviewTitle'].values:
  def cleanldigit(sent): #function to clean the word of any digit
     cleand = re.compile('\w*\d\w*')
     cleandigit= re.sub(cleand, ' ', sent)
     return cleandigit


for sent in product_data['ReviewTitle'].values:
  def cleanhtml(sent): #function to clean the word of any html-tags
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, ' ', sent)
    return cleantext
for sent in product_data['ReviewTitle'].values:
  def cleanpunc(sent): #function to clean the word of any punctuation or special characters
    cleaned = re.sub(r'[?|!|\'|"|#]',r'',sent)
    cleaned = re.sub(r'[.|,|)|(|\|/]',r' ',cleaned)
    return  cleaned
nltk.download('stopwords')
from nltk.corpus import stopwords
sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer which is developed in recent years
stop=set(stopwords.words('english'))



    


print(stop)
print('************************************')
print(sno.stem('tasty'))

i=0
str1=' '
final_string=[]
all_positive_words=[] # store words from +ve reviews here
all_negative_words=[] # store words from -ve reviews here.
s=''
for sent in product_data['ReviewTitle'].values:
    filtered_sentence=[]
    #print(sent);
    sent=cleanhtml(sent) # remove HTMl tags
    sent=cleandurl(sent) # remove URL
    sent=cleanldigit(sent) # remove digits


    
    for w in sent.split():

        for cleaned_words in cleanpunc(w).split():
            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    
                if(cleaned_words.lower() not in stop):
                    s=(sno.stem(cleaned_words.lower())).encode('utf8')
                    filtered_sentence.append(s)
                    if (product_data['Rating'].values)[i] == 'positive': 
                        all_positive_words.append(s) #list of all words used to describe positive reviews
                    if(product_data['Rating'].values)[i] == 'negative':
                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews
                else:
                    continue
            else:
                continue 
    print(filtered_sentence)
    str1 = b" ".join(filtered_sentence) #final string of cleaned words
    print("***********************************************************************")
    
    final_string.append(str1)
    i+=1

product_data['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review 
product_data['CleanedText']=product_data['CleanedText'].str.decode("utf-8")

product_data

"""Feature Engineering
**we will be using Textblob for Sentement_polarity of the Reviews and word len and review len**
"""

from textblob import TextBlob   ####TextBlob is a python library and offers a simple API to access its methods and perform basic NLP tasks. 
import nltk
nltk.download('vader_lexicon')

!pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
vader = SentimentIntensityAnalyzer()

def get_polarity(x):
    return TextBlob(x).sentiment.polarity
product_data['Polarity'] = product_data['ReviewTitle'].apply(get_polarity)
product_data['Sentiment_Type']=''
product_data.loc[product_data.Polarity>0,'Sentiment_Type']='POSITIVE'
product_data.loc[product_data.Polarity==0,'Sentiment_Type']='NEUTRAL'
product_data.loc[product_data.Polarity<0,'Sentiment_Type']='NEGATIVE'

product_data.reset_index(drop=True)

def configure_plotly_browser_state():
  import IPython
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
        <script>
          requirejs.config({
            paths: {
              base: '/static/base',
              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',
            },
          });
        </script>
        '''))

"""**Distribution of Sentiment polarity**"""

configure_plotly_browser_state()
import plotly.graph_objs as go
import plotly.offline as pyo
from plotly.offline import init_notebook_mode, iplot
pyo.init_notebook_mode()
init_notebook_mode(connected=True) 
product_data['Polarity'].iplot(kind='hist',colors='red',bins=50,xTitle='Polarity',yTitle='count',title='Sentiment Polarity distribution')

######### we will be ploting polarity count on reviews

####### we will be ploting count of sentiment_type 
configure_plotly_browser_state()
product_data.Sentiment_Type.value_counts().iplot(kind='bar',title="Sentiment Analysis")

###### we will be plotting rating count

configure_plotly_browser_state()
product_data['Rating'].iplot(kind='hist',colors='red',bins=50,xTitle='Rating',yTitle='count',title='Review Rating distribution')

product_data['Brand'].value_counts()

product_data.groupby('Brand').count()

###COUNT PLOT ON BRANDS
configure_plotly_browser_state()
product_data['Brand'].value_counts().iplot(kind='bar',xTitle='Brand',yTitle='count',title='COUNT OF BRANDS')

!pip install text2emotion
import text2emotion as te

def get_emotion(x):
         return te.get_emotion(x)

product_data.loc[product_data.Polarity>=0,'Review_emotions']='HAPPY'
product_data.loc[product_data.Polarity<0,'Review_emotions']='ANGRY'
product_data.loc[product_data.Polarity==0,'Review_emotions']='SATISFIED'

product_data.loc[product_data.Polarity>=0,'pol_num']='1'
product_data.loc[product_data.Polarity<0,'pol_num']='-1'
product_data.loc[product_data.Polarity==0,'pol_num']='0'

product_data.head(50)

from sklearn.feature_extraction.text import CountVectorizer

sns.pairplot(product_data)

sns.catplot(x='Brand',y='Polarity',data=product_data,kind='box',height=4, aspect=3)

import plotly. express as px
import plotly.graph_objects as go

"""# **Based on Rating and Polarity  Recomending Brands **

---


"""

x1=product_data[product_data['Rating']>=3.0]['Brand']
x2=product_data[product_data['Rating']<=2.0]['Brand']

trace1=go.Histogram(x=x1,name='Recomended',opacity=0.8)
trace2=go.Histogram(x=x2,name='Not Recomended',opacity=0.8)

configure_plotly_browser_state()
data=[trace1,trace2]
layout=go.Layout(barmode='overlay',title='Distribution of Recomending Brands based on Ratings')
fig=go.Figure(data=data,layout=layout)
fig.show()

y1=product_data[product_data['Polarity']>=0]['Brand']
y2=product_data[product_data['Polarity']<0]['Brand']
trace_1=go.Histogram(x=y1,name='Recomended',opacity=0.8)
trace_2=go.Histogram(x=y2,name='Not Recomended',opacity=0.8)

s=pd.concat([y1, y2])
s

configure_plotly_browser_state()
data=[trace_1,trace_2]
layout=go.Layout(barmode='overlay',title='Distribution of polarity of reviews based on ratings')
fig=go.Figure(data=data,layout=layout)
fig.show()

!pip install autoviz

from autoviz.AutoViz_Class import AutoViz_Class

AV = AutoViz_Class()

filename = "Comp Product Reviews.csv"
sep = ","
dft = AV.AutoViz(
    filename,
    sep=",",
    depVar="",
    dfte=None,
    header=0,
    verbose=0,
    lowess=False,
    chart_format="svg",
    max_rows_analyzed=150000,
    max_cols_analyzed=30,
)





# highest positive sentiment reviews 
only_pos=product_data[product_data["Polarity"] >0].sort_values("Sentiment_Type", ascending = False)[["CleanedText", "Sentiment_Type","Brand","Polarity"]]

# highest Nutral sentiment reviews
only_Nutral=product_data[product_data["Polarity"] ==0].sort_values("Sentiment_Type", ascending = False)[["CleanedText", "Sentiment_Type","Brand","Polarity"]]

# highest Negative sentiment reviews 
only_Neg=product_data[product_data["Polarity"] <0].sort_values("Sentiment_Type", ascending = False)[["CleanedText", "Sentiment_Type","Brand","Polarity"]]

only_pos

import gensim
from gensim import corpora

from nltk.stem.wordnet import WordNetLemmatizer
#clean the data
stop = set(stopwords.words('english'))
exclude = set(string.punctuation)
lemma_Pos = WordNetLemmatizer()

def clean(text):
    stop_free = ' '.join([word for word in text.lower().split() if word not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = ' '.join([lemma_Pos.lemmatize(word) for word in punc_free.split()])
    return normalized.split()

only_pos['lemma_Pos']=only_pos['CleanedText'].apply(clean)
only_pos['lemma_Pos']

# Commented out IPython magic to ensure Python compatibility.
#create dictionary
dictionary_pos = corpora.Dictionary(only_pos['lemma_Pos'])
#Total number of non-zeroes in the BOW matrix (sum of the number of unique words per document over the entire corpus).
print(dictionary_pos.num_nnz)
print("*********************************************************************************************************")
#create document term matrix
doc_term_matrix_p = [dictionary_pos.doc2bow(doc) for doc in only_pos['lemma_Pos'] ]
print(len(doc_term_matrix_p))
print("*********************************************************************************************************")
lda = gensim.models.ldamodel.LdaModel
num_topics=3
# %time ldamodel_pos = lda(doc_term_matrix_p,num_topics=num_topics,id2word=dictionary_pos,passes=50,minimum_probability=0)
ldamodel_pos.print_topics(num_topics=num_topics)

lda_display_pos = pyLDAvis.gensim.prepare(ldamodel_pos, doc_term_matrix_p, dictionary_pos, sort_topics=False, mds='mmds')
pyLDAvis.display(lda_display_pos)

# Assigns the topics to the documents in corpus
lda_corpus_p = ldamodel_pos[doc_term_matrix_p]
[doc for doc in lda_corpus_p]

scores = list(chain(*[[score for topic_id,score in topic] \
                      for topic in [doc for doc in lda_corpus_p]]))

threshold = sum(scores)/len(scores)
print(threshold)
cluster1_p = [j for i,j in zip(lda_corpus,only_pos.index) if i[0][1] > threshold]
cluster2_p = [j for i,j in zip(lda_corpus,only_pos.index) if i[1][1] > threshold]
cluster3_p = [j for i,j in zip(lda_corpus,only_pos.index) if i[2][1] > threshold]

print(len(cluster1_p))
print(len(cluster2_p))
print(len(cluster3_p))

"""**Negative**"""

stop = set(stopwords.words('english'))
exclude = set(string.punctuation)
lemma_neg = WordNetLemmatizer()

def clean(text):
    stop_free = ' '.join([word for word in text.lower().split() if word not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = ' '.join([lemma_neg.lemmatize(word) for word in punc_free.split()])
    return normalized.split()

only_Neg['lemma_neg']=only_Neg['CleanedText'].apply(clean)
only_Neg['lemma_neg']

# Commented out IPython magic to ensure Python compatibility.
#create dictionary
Neg_dictionary = corpora.Dictionary(only_Neg['lemma_neg'])
#Total number of non-zeroes in the BOW matrix (sum of the number of unique words per document over the entire corpus).
print(Neg_dictionary.num_nnz)
print("*********************************************************************************************************")
#create document term matrix
doc_term_matrix_N = [Neg_dictionary.doc2bow(doc) for doc in only_Neg['lemma_neg'] ]
print(len(doc_term_matrix_N))
print("*********************************************************************************************************")
lda_neg = gensim.models.ldamodel.LdaModel
num_topics=3
# %time ldamodel_Neg = lda_neg(doc_term_matrix_N,num_topics=num_topics,id2word=Neg_dictionary,passes=50,minimum_probability=0)
ldamodel_Neg.print_topics(num_topics=num_topics)

lda_display_Neg = pyLDAvis.gensim.prepare(ldamodel_Neg, doc_term_matrix_N, Neg_dictionary, sort_topics=False, mds='mmds')
pyLDAvis.display(lda_display_Neg)

# Assigns the topics to the documents in corpus
lda_corpus_Neg= ldamodel_Neg[doc_term_matrix_N]
[doc for doc in lda_corpus_Neg]

scores = list(chain(*[[score for topic_id,score in topic] \
                      for topic in [doc for doc in lda_corpus_Neg]]))

threshold = sum(scores)/len(scores)
print(threshold)
cluster1_N = [j for i,j in zip(lda_corpus_Neg,only_Neg.index) if i[0][1] > threshold]
cluster2_N = [j for i,j in zip(lda_corpus_Neg,only_Neg.index) if i[1][1] > threshold]
cluster3_N = [j for i,j in zip(lda_corpus_Neg,only_Neg.index) if i[2][1] > threshold]

print(len(cluster1_N))
print(len(cluster2_N))
print(len(cluster3_N))

stop = set(stopwords.words('english'))
exclude = set(string.punctuation)
lemma_nutral = WordNetLemmatizer()

def clean(text):
    stop_free = ' '.join([word for word in text.lower().split() if word not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = ' '.join([lemma_nutral.lemmatize(word) for word in punc_free.split()])
    return normalized.split()

only_Nutral['lemma_nutral']=only_Nutral['CleanedText'].apply(clean)
only_Nutral['lemma_nutral']

# Commented out IPython magic to ensure Python compatibility.
#create dictionary
Nuu_dictionary = corpora.Dictionary(only_Nutral['lemma_nutral'])
#Total number of non-zeroes in the BOW matrix (sum of the number of unique words per document over the entire corpus).
print(Nuu_dictionary.num_nnz)
print("*********************************************************************************************************")
#create document term matrix
doc_term_matrix_Nuu = [Nuu_dictionary.doc2bow(doc) for doc in only_Nutral['lemma_nutral'] ]
print(len(doc_term_matrix_Nuu))
print("*********************************************************************************************************")
lda_nutr = gensim.models.ldamodel.LdaModel
num_topics=3
# %time ldamodel_Nut= lda_nutr(doc_term_matrix_Nuu,num_topics=num_topics,id2word=Nuu_dictionary,passes=50,minimum_probability=0)
ldamodel_Nut.print_topics(num_topics=num_topics)

lda_display_Nut = pyLDAvis.gensim.prepare(ldamodel_Nut, doc_term_matrix_Nuu, Nuu_dictionary, sort_topics=False, mds='mmds')
pyLDAvis.display(lda_display_Nut)

# Assigns the topics to the documents in corpus
lda_corpus_Nut= ldamodel_Nut[doc_term_matrix_Nuu]
[doc for doc in lda_corpus_Nut]

scores = list(chain(*[[score for topic_id,score in topic] \
                      for topic in [doc for doc in lda_corpus_Nut]]))

threshold = sum(scores)/len(scores)
print(threshold)
cluster1_Nut = [j for i,j in zip(lda_corpus_Nut,only_Nutral.index) if i[0][1] > threshold]
cluster2_Nut = [j for i,j in zip(lda_corpus_Nut,only_Nutral.index) if i[1][1] > threshold]
cluster3_Nut = [j for i,j in zip(lda_corpus_Nut,only_Nutral.index) if i[2][1] > threshold]

print(len(cluster1_Nut))
print(len(cluster2_Nut))
print(len(cluster3_Nut))





configure_plotly_browser_state()
only_pos['Brand'].value_counts().iplot(kind='bar',xTitle='Brand',yTitle='count',title='COUNT OF Positive Reviews')

configure_plotly_browser_state()
only_Nutral['Brand'].value_counts().iplot(kind='bar',xTitle='Brand',yTitle='count',title='COUNT OF Nutral Reviews ')

configure_plotly_browser_state()
only_Neg['Brand'].value_counts().iplot(kind='bar',xTitle='Brand',yTitle='count',title='COUNT OF Negative Reviews')



! pip install wordcloud

from wordcloud import WordCloud

import statsmodels.formula.api as sm
t1=product_data[product_data.Polarity>0]
t2=product_data[product_data.Polarity==0]
t3=product_data[product_data.Polarity<0]
reviews_sample = pd.concat([t1,t2,t3],axis=0)
reviews_sample.reset_index(drop=True,inplace=True)

#Wordcloud function's input needs to be a single string of text.
# concatenating all Summaries into a single string.
# similarly you can build for Text column
reviews_str = reviews_sample.CleanedText.str.cat()
wordcloud = WordCloud(background_color='white').generate(reviews_str)
plt.figure(figsize=(10,10))
plt.imshow(wordcloud,interpolation='bilinear')
plt.axis("off")
plt.show()

# Now let's split the data into Negative (Score is 1 or 2) and Positive (4 or #5) Reviews.
negative_reviews = reviews_sample[reviews_sample['Rating'].isin([1,2]) ]
positive_reviews = reviews_sample[reviews_sample['Rating'].isin([3,4,5]) ]
# Transform to single string
negative_reviews_str = negative_reviews.CleanedText.str.cat()
positive_reviews_str = positive_reviews.CleanedText.str.cat()
wordcloud_negative = WordCloud(background_color='gray').generate(negative_reviews_str)
wordcloud_positive = WordCloud(background_color='black').generate(positive_reviews_str)
# Plot
fig = plt.figure(figsize=(10,10))
ax1 = fig.add_subplot(211)
ax1.imshow(wordcloud_negative,interpolation='bilinear')
ax1.axis("off")
ax1.set_title('Reviews with Negative Rating',fontsize=20)

fig = plt.figure(figsize=(10,10))
ax2 = fig.add_subplot(212)
ax2.imshow(wordcloud_positive,interpolation='bilinear')
ax2.axis("off")
ax2.set_title('Reviews with Positive Rating',fontsize=20)
plt.show()

product_data.reset_index(inplace=True)

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
corpus = []
for i in range(0, len(product_data)):
    review = re.sub('[^a-zA-Z]', ' ', product_data['CleanedText'][i])
    review = review.lower()
    review = review.split()
    
    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)

## Applying Countvectorizer
# Creating the Bag of Words model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=5000,ngram_range=(1,3))
X = cv.fit_transform(corpus).toarray()

X.shape

y=product_data['pol_num']

## Divide the dataset into Train and Test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)

from sklearn.naive_bayes import MultinomialNB
classifier=MultinomialNB()

from sklearn import metrics
import numpy as np
import itertools



classifier.fit(X_train, y_train)
pred_NB = classifier.predict(X_test)
from sklearn.metrics import accuracy_score, precision_score, recall_score
print('Accuracy score of NB: ', accuracy_score(y_test, pred_NB)*100)
cm = metrics.confusion_matrix(y_test, pred_NB)
cm

print("Classification Report\n",classification_report(y_test, pred_NB))

"""**SVM**"""

#Import svm model
from sklearn import svm

#Create a svm Classifier
clf = svm.SVC(kernel='linear') #  Linear 

#Train the model using the training sets
clf.fit(X_train, y_train)

y_predsvm= clf.predict(X_test)
print(y_predsvm)

print('Accuracy score of NB: ', accuracy_score(y_test, y_predsvm)*100)
con_M = metrics.confusion_matrix(y_test, y_predsvm)
cm

print("Classification Report\n",classification_report(y_test, y_predsvm))

from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
# create dataset
X_train, y_train = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)
# prepare the cross-validation procedure
cv = KFold(n_splits=10, random_state=1, shuffle=True)
# evaluate model
scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))

## Applying Countvectorizer
# Creating the Bag of Words model
from sklearn.feature_extraction.text import CountVectorizer
#Initialize the TF-IDF vectorizer
tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,max_df = 0.7,norm='l2', encoding='latin-1', ngram_range=(1, 2),
stop_words='english')
X_tfidf = tfidf.fit_transform(product_data['CleanedText'])
print("After vectorized text data\n\n",X_tfidf)

x_train, x_test, Y_train,Y_test = train_test_split(X_tfidf, y, test_size=0.1, random_state=0)

clf_t = LinearSVC()

#Fit train and test into the model
clf_t.fit(x_train, Y_train)

#Predict the result
y_pred = clf_t.predict(x_test)

print("Confusion Matrix\n",confusion_matrix(Y_test,y_pred))

print("Accuracy of SVC: ",accuracy_score(Y_test,y_pred)*100)
print("Classification Report\n",classification_report(Y_test,y_pred))

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

rf_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
rf_classifier.fit(x_train, Y_train)
y_pred_rf = rf_classifier.predict(x_test)

cm_RandFor = confusion_matrix(Y_test, y_pred_rf)
cm_RandFor

print("Accuracy of RF: ",accuracy_score(Y_test,y_pred_rf)*100)
print("Classification Report\n",classification_report(Y_test,y_pred_rf))

"""### **NBC on TF-IDF**


"""

classifier.fit(x_train, Y_train)
pred_NB_T = classifier.predict(x_test)
from sklearn.metrics import accuracy_score, precision_score, recall_score
print('Accuracy score of NB: ', accuracy_score(Y_test, pred_NB_T)*100)
cm_T = metrics.confusion_matrix(Y_test, pred_NB_T)
cm_T

print("Classification Report\n",classification_report(Y_test, pred_NB_T))



